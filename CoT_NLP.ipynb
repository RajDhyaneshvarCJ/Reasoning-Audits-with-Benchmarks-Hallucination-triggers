{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcFS7OLyC9UftLhBiUVLQR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajDhyaneshvarCJ/Reasoning-Audits-with-Benchmarks-Hallucination-triggers/blob/main/CoT_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "4Fv2HwG_9xBB",
        "outputId": "6a1f5276-956c-492d-e984-84d6f931165f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded samples: 1000\n",
            "Processing 1/1000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'\\n  \"cot\"'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-129455309.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mcot_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_cot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# hallucination judge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-129455309.py\u001b[0m in \u001b[0;36mgenerate_cot\u001b[0;34m(context, question, clean)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_cot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCOT_PROMPT_CLEAN\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclean\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mCOT_PROMPT_MESSY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mformatted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '\\n  \"cot\"'"
          ]
        }
      ],
      "source": [
        "# # import & config\n",
        "# import json\n",
        "# import time\n",
        "# import uuid\n",
        "# from datetime import datetime\n",
        "# from openai import OpenAI\n",
        "# from google.colab import userdata\n",
        "\n",
        "# API_KEY = userdata.get('openai_nlp_key')\n",
        "# if not API_KEY:\n",
        "#     raise ValueError(\"key not found in secrets\")\n",
        "\n",
        "# client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "# MODEL_ANSWER = \"gpt-4o-mini\"\n",
        "# MODEL_JUDGE = \"gpt-4.1\"           # Stronger judge\n",
        "# TEMPERATURE_CLEAN = 0.2\n",
        "# TEMPERATURE_MESSY = 0.8\n",
        "\n",
        "# RAW_DATASET_PATH = \"dataset_raw.jsonl\"\n",
        "\n",
        "# # OUTPUT_PATH = \"dataset_with_cot.jsonl\" for full run\n",
        "# OUTPUT_PATH = \"dataset_with_cot_sample_runs.jsonl\"\n",
        "\n",
        "\n",
        "# # load ds\n",
        "# def load_raw_dataset(path):\n",
        "#     with open(path, \"r\") as f:\n",
        "#         return [json.loads(line) for line in f]\n",
        "\n",
        "# raw_data = load_raw_dataset(RAW_DATASET_PATH)\n",
        "# print(\"Loaded samples:\", len(raw_data))\n",
        "\n",
        "\n",
        "# # prompt for CoT\n",
        "# COT_PROMPT_CLEAN = \"\"\"\n",
        "# You are a serious reasoning assistant.\n",
        "\n",
        "# Given the context and question, produce:\n",
        "# 1. very short chain of thought in steps as a JSON array\n",
        "# 2. a final answer\n",
        "\n",
        "# Return JSON only:\n",
        "# {\n",
        "#   \"cot\": [\"step 1...\", \"step 2...\"],\n",
        "#   \"answer\": \"...\"\n",
        "# }\n",
        "\n",
        "# Context:\n",
        "# {context}\n",
        "\n",
        "# Question:\n",
        "# {question}\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# COT_PROMPT_MESSY = \"\"\"\n",
        "# Explain your reasoning in multiple steps before answering.\n",
        "\n",
        "# Return JSON only:\n",
        "# {\n",
        "#   \"cot\": [\"step 1...\", \"step 2...\"],\n",
        "#   \"answer\": \"...\"\n",
        "# }\n",
        "\n",
        "# Context:\n",
        "# {context}\n",
        "\n",
        "# Question:\n",
        "# {question}\n",
        "# \"\"\"\n",
        "\n",
        "# # llm call for answers\n",
        "# def generate_cot(context, question, clean=True):\n",
        "#     prompt = (COT_PROMPT_CLEAN if clean else COT_PROMPT_MESSY)\n",
        "#     formatted = prompt.format(context=context, question=question)\n",
        "\n",
        "#     try:\n",
        "#         response = client.chat.completions.create(\n",
        "#             model=MODEL_ANSWER,\n",
        "#             messages=[{\"role\": \"user\", \"content\": formatted}],\n",
        "#             temperature=TEMPERATURE_CLEAN if clean else TEMPERATURE_MESSY,\n",
        "#         )\n",
        "#         return json.loads(response.choices[0].message.content.strip())\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(\"Error:\", e)\n",
        "#         return {\"cot\": [], \"answer\": \"ERROR\"}\n",
        "\n",
        "\n",
        "\n",
        "# # hallucination judge using gpt-4\n",
        "# JUDGE_PROMPT = \"\"\"\n",
        "# You are a hallucination detection judge.\n",
        "\n",
        "# Given:\n",
        "# Context: {context}\n",
        "# Question: {question}\n",
        "# Answer: {answer}\n",
        "# Reasoning steps: {cot}\n",
        "\n",
        "# Determine if the answer is:\n",
        "# - fully supported by the context,\n",
        "# - contradicted by the context,\n",
        "# - fabricated,\n",
        "# - partially correct but contains hallucinations,\n",
        "# - unsupported speculation.\n",
        "\n",
        "# Return JSON ONLY:\n",
        "# {\n",
        "#   \"label\": \"0\" | \"0.5\" | \"1\",\n",
        "#   \"score\": float\n",
        "# }\n",
        "# \"\"\"\n",
        "\n",
        "# # judge call\n",
        "# def hallucination_judge_llm(context, question, answer, cot_steps):\n",
        "#     formatted = JUDGE_PROMPT.format(\n",
        "#         context=context,\n",
        "#         question=question,\n",
        "#         answer=answer,\n",
        "#         cot=json.dumps(cot_steps)\n",
        "#     )\n",
        "\n",
        "#     try:\n",
        "#         response = client.chat.completions.create(\n",
        "#             model=MODEL_JUDGE,\n",
        "#             messages=[{\"role\": \"user\", \"content\": formatted}],\n",
        "#             temperature=0.0,\n",
        "#         )\n",
        "#         return json.loads(response.choices[0].message.content.strip())\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(\"Error in judge:\", e)\n",
        "#         return {\n",
        "#             \"label\": \"JUDGE_ERROR\",\n",
        "#             \"score\": -1\n",
        "#         }\n",
        "\n",
        "\n",
        "\n",
        "# # main loop\n",
        "# output_rows = []\n",
        "\n",
        "# for i, item in enumerate(raw_data):\n",
        "#     if i == 10:\n",
        "#         break\n",
        "# # for i, item in enumerate(raw_data): for full run only\n",
        "#     print(f\"Processing {i+1}/{len(raw_data)}\")\n",
        "\n",
        "#     context = item.get(\"context\", \"\")\n",
        "#     question = item.get(\"question\", \"\")\n",
        "#     source = item.get(\"source\", \"\")\n",
        "#     cot_output = generate_cot(context, question, clean=True)\n",
        "\n",
        "#     # hallucination judge\n",
        "#     judge_result = hallucination_judge_llm(\n",
        "#         context,\n",
        "#         question,\n",
        "#         cot_output[\"answer\"],\n",
        "#         cot_output[\"cot\"]\n",
        "#     )\n",
        "\n",
        "#     row = {\n",
        "#         \"id\": item.get(\"id\", str(uuid.uuid4())),\n",
        "#         \"source\": source,\n",
        "#         \"context\": context,\n",
        "#         \"question\": question,\n",
        "#         \"cot_steps\": cot_output[\"cot\"],\n",
        "#         \"answer\": cot_output[\"answer\"],\n",
        "#         \"hallucination_label\": judge_result[\"label\"],\n",
        "#         \"hallucination_score\": judge_result[\"score\"],\n",
        "#         \"metadata\": {\n",
        "#             \"model_answer\": MODEL_ANSWER,\n",
        "#             \"model_judge\": MODEL_JUDGE,\n",
        "#             \"temperature\": TEMPERATURE_CLEAN,\n",
        "#             \"timestamp\": str(datetime.utcnow()),\n",
        "#         }\n",
        "#     }\n",
        "\n",
        "#     output_rows.append(row)\n",
        "#     time.sleep(0.4)\n",
        "\n",
        "\n",
        "# # save\n",
        "# with open(OUTPUT_PATH, \"w\") as f:\n",
        "#     for row in output_rows:\n",
        "#         f.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "# print(\"Saved:\", OUTPUT_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import & config\n",
        "import json\n",
        "import time\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get('openai_nlp_key')\n",
        "if not API_KEY:\n",
        "    raise ValueError(\"key not found in secrets\")\n",
        "\n",
        "client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "MODEL_ANSWER = \"gpt-4o-mini\"\n",
        "TEMPERATURE_CLEAN = 0.2\n",
        "TEMPERATURE_MESSY = 0.8\n",
        "\n",
        "RAW_DATASET_PATH = \"dataset_raw.jsonl\"\n",
        "\n",
        "# OUTPUT_PATH = \"dataset_with_cot.jsonl\" for full run\n",
        "OUTPUT_PATH = \"dataset_with_cot_unjudged.jsonl\"\n",
        "\n",
        "\n",
        "# load ds\n",
        "def load_raw_dataset(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "raw_data = load_raw_dataset(RAW_DATASET_PATH)\n",
        "print(\"Loaded samples:\", len(raw_data))\n",
        "\n",
        "\n",
        "# prompt for CoT\n",
        "COT_PROMPT_CLEAN = \"\"\"\n",
        "You are a serious reasoning assistant.\n",
        "\n",
        "Given the context and question, produce:\n",
        "1. chain of thought in steps as a JSON array\n",
        "2. a final answer\n",
        "\n",
        "Return JSON only:\n",
        "{{\n",
        "  \"cot\": [\"step 1...\", \"step 2...\"],\n",
        "  \"answer\": \"...\"\n",
        "}}\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "COT_PROMPT_MESSY = \"\"\"\n",
        "Explain your reasoning in multiple steps before answering.\n",
        "\n",
        "Return JSON only:\n",
        "{{}\n",
        "  \"cot\": [\"step 1...\", \"step 2...\"],\n",
        "  \"answer\": \"...\"\n",
        "}}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "# llm call for answers\n",
        "def generate_cot(context, question, clean=True):\n",
        "    prompt = (COT_PROMPT_CLEAN if clean else COT_PROMPT_MESSY)\n",
        "    formatted = prompt.format(context=context, question=question)\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_ANSWER,\n",
        "            messages=[{\"role\": \"user\", \"content\": formatted}],\n",
        "            temperature=TEMPERATURE_CLEAN if clean else TEMPERATURE_MESSY,\n",
        "        )\n",
        "        return json.loads(response.choices[0].message.content.strip())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        return {\"cot\": [], \"answer\": \"ERROR\"}\n",
        "\n",
        "\n",
        "\n",
        "# main loop\n",
        "output_rows = []\n",
        "\n",
        "for i, item in enumerate(raw_data):\n",
        "    if i == 100:\n",
        "        break\n",
        "# for i, item in enumerate(raw_data): for full run only\n",
        "    print(f\"Processing {i+1}/{len(raw_data)}\")\n",
        "\n",
        "    context = item.get(\"context\", \"\")\n",
        "    question = item.get(\"question\", \"\")\n",
        "    source = item.get(\"source\", \"\")\n",
        "    cot_output = generate_cot(context, question, clean=True)\n",
        "\n",
        "    row = {\n",
        "        \"id\": item.get(\"id\", str(uuid.uuid4())),\n",
        "        \"source\": source,\n",
        "        \"context\": context,\n",
        "        \"question\": question,\n",
        "        \"cot_steps\": cot_output[\"cot\"],\n",
        "        \"answer\": cot_output[\"answer\"],\n",
        "        \"metadata\": {\n",
        "            \"model_answer\": MODEL_ANSWER,\n",
        "            \"temperature\": TEMPERATURE_CLEAN,\n",
        "            \"timestamp\": str(datetime.utcnow()),\n",
        "        }\n",
        "    }\n",
        "\n",
        "    output_rows.append(row)\n",
        "    time.sleep(0.4)\n",
        "\n",
        "\n",
        "# save\n",
        "with open(OUTPUT_PATH, \"w\") as f:\n",
        "    for row in output_rows:\n",
        "        f.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "print(\"Saved:\", OUTPUT_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwDOErLgJpXB",
        "outputId": "ff22bed3-28c9-4a8e-e30b-f6c1400e2cfd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded samples: 1000\n",
            "Processing 1/1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2669721822.py:114: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"timestamp\": str(datetime.utcnow()),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 2/1000\n",
            "Processing 3/1000\n",
            "Processing 4/1000\n",
            "Processing 5/1000\n",
            "Processing 6/1000\n",
            "Processing 7/1000\n",
            "Processing 8/1000\n",
            "Processing 9/1000\n",
            "Processing 10/1000\n",
            "Processing 11/1000\n",
            "Processing 12/1000\n",
            "Processing 13/1000\n",
            "Processing 14/1000\n",
            "Processing 15/1000\n",
            "Processing 16/1000\n",
            "Processing 17/1000\n",
            "Processing 18/1000\n",
            "Processing 19/1000\n",
            "Processing 20/1000\n",
            "Processing 21/1000\n",
            "Processing 22/1000\n",
            "Processing 23/1000\n",
            "Processing 24/1000\n",
            "Processing 25/1000\n",
            "Processing 26/1000\n",
            "Processing 27/1000\n",
            "Processing 28/1000\n",
            "Processing 29/1000\n",
            "Processing 30/1000\n",
            "Processing 31/1000\n",
            "Processing 32/1000\n",
            "Processing 33/1000\n",
            "Processing 34/1000\n",
            "Processing 35/1000\n",
            "Processing 36/1000\n",
            "Processing 37/1000\n",
            "Processing 38/1000\n",
            "Processing 39/1000\n",
            "Processing 40/1000\n",
            "Processing 41/1000\n",
            "Processing 42/1000\n",
            "Processing 43/1000\n",
            "Processing 44/1000\n",
            "Processing 45/1000\n",
            "Processing 46/1000\n",
            "Processing 47/1000\n",
            "Processing 48/1000\n",
            "Processing 49/1000\n",
            "Processing 50/1000\n",
            "Processing 51/1000\n",
            "Processing 52/1000\n",
            "Processing 53/1000\n",
            "Processing 54/1000\n",
            "Processing 55/1000\n",
            "Processing 56/1000\n",
            "Processing 57/1000\n",
            "Processing 58/1000\n",
            "Processing 59/1000\n",
            "Processing 60/1000\n",
            "Processing 61/1000\n",
            "Processing 62/1000\n",
            "Processing 63/1000\n",
            "Processing 64/1000\n",
            "Processing 65/1000\n",
            "Processing 66/1000\n",
            "Processing 67/1000\n",
            "Processing 68/1000\n",
            "Processing 69/1000\n",
            "Processing 70/1000\n",
            "Processing 71/1000\n",
            "Processing 72/1000\n",
            "Processing 73/1000\n",
            "Processing 74/1000\n",
            "Processing 75/1000\n",
            "Processing 76/1000\n",
            "Processing 77/1000\n",
            "Processing 78/1000\n",
            "Processing 79/1000\n",
            "Processing 80/1000\n",
            "Processing 81/1000\n",
            "Processing 82/1000\n",
            "Processing 83/1000\n",
            "Processing 84/1000\n",
            "Processing 85/1000\n",
            "Processing 86/1000\n",
            "Processing 87/1000\n",
            "Processing 88/1000\n",
            "Processing 89/1000\n",
            "Processing 90/1000\n",
            "Processing 91/1000\n",
            "Processing 92/1000\n",
            "Processing 93/1000\n",
            "Processing 94/1000\n",
            "Processing 95/1000\n",
            "Processing 96/1000\n",
            "Processing 97/1000\n",
            "Processing 98/1000\n",
            "Processing 99/1000\n",
            "Processing 100/1000\n",
            "Saved: dataset_with_cot_unjudged.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JUDGE"
      ],
      "metadata": {
        "id": "hKXRC4nUKLml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import & config\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get('openai_nlp_key')\n",
        "if not API_KEY:\n",
        "    raise ValueError(\"key not found in secrets\")\n",
        "\n",
        "client = OpenAI(api_key=API_KEY)\n",
        "\n",
        "MODEL_JUDGE = \"gpt-4.1\"\n",
        "\n",
        "INPUT_PATH = \"dataset_with_cot_unjudged.jsonl\"\n",
        "OUTPUT_PATH = \"dataset_with_cot_judged.jsonl\"\n",
        "\n",
        "\n",
        "# load dataset\n",
        "def load_dataset(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "data = load_dataset(INPUT_PATH)\n",
        "print(\"Loaded for judging:\", len(data))\n",
        "\n",
        "\n",
        "\n",
        "# hallucination judge using gpt-4\n",
        "JUDGE_PROMPT = \"\"\"\n",
        "You are a hallucination detection judge.\n",
        "\n",
        "Given:\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer: {answer}\n",
        "Reasoning steps: {cot}\n",
        "\n",
        "Determine if the answer is:\n",
        "- fully supported by the context,\n",
        "- contradicted by the context,\n",
        "- fabricated,\n",
        "- Only mark ‘1’ if the answer introduces factual content not present in the context.,\n",
        "- if the model had smoothen the grammar or rewrites without introducing new factual data, it is not a hallucination\n",
        "- give \"0.5\" if it seems right but got the context jumbled up,\n",
        "- unsupported speculation.\n",
        "\n",
        "Return JSON ONLY:\n",
        "{{\n",
        "  \"label\": \"0\" | \"0.5\" | \"1\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# judge call\n",
        "def hallucination_judge_llm(context, question, answer, cot_steps):\n",
        "    formatted = JUDGE_PROMPT.format(\n",
        "        context=context,\n",
        "        question=question,\n",
        "        answer=answer,\n",
        "        cot=json.dumps(cot_steps)\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_JUDGE,\n",
        "            messages=[{\"role\": \"user\", \"content\": formatted}],\n",
        "            temperature=0.0,\n",
        "        )\n",
        "        return json.loads(response.choices[0].message.content.strip())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error in judge:\", e)\n",
        "        return {\"label\": \"JUDGE_ERROR\"}\n",
        "\n",
        "\n",
        "\n",
        "# judging loop\n",
        "judged_rows = []\n",
        "\n",
        "for i, item in enumerate(data):\n",
        "    print(f\"Judging {i+1}/{len(data)}\")\n",
        "\n",
        "    judge_result = hallucination_judge_llm(\n",
        "        item[\"context\"],\n",
        "        item[\"question\"],\n",
        "        item[\"answer\"],\n",
        "        item[\"cot_steps\"]\n",
        "    )\n",
        "\n",
        "    item[\"hallucination_label\"] = judge_result[\"label\"]\n",
        "    item[\"metadata\"][\"model_judge\"] = MODEL_JUDGE\n",
        "    item[\"metadata\"][\"judged_timestamp\"] = str(datetime.utcnow())\n",
        "\n",
        "    judged_rows.append(item)\n",
        "    time.sleep(0.4)\n",
        "\n",
        "\n",
        "\n",
        "# save\n",
        "with open(OUTPUT_PATH, \"w\") as f:\n",
        "    for row in judged_rows:\n",
        "        f.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "print(\"judged dataset:\", OUTPUT_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27FyEvlCKMwa",
        "outputId": "7ea1f0b7-60fc-4dbc-edee-8c038499f2ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded for judging: 100\n",
            "Judging 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3179100920.py:93: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  item[\"metadata\"][\"judged_timestamp\"] = str(datetime.utcnow())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Judging 2/100\n",
            "Judging 3/100\n",
            "Judging 4/100\n",
            "Judging 5/100\n",
            "Judging 6/100\n",
            "Judging 7/100\n",
            "Judging 8/100\n",
            "Judging 9/100\n",
            "Judging 10/100\n",
            "Judging 11/100\n",
            "Judging 12/100\n",
            "Judging 13/100\n",
            "Judging 14/100\n",
            "Judging 15/100\n",
            "Judging 16/100\n",
            "Judging 17/100\n",
            "Judging 18/100\n",
            "Judging 19/100\n",
            "Judging 20/100\n",
            "Judging 21/100\n",
            "Judging 22/100\n",
            "Judging 23/100\n",
            "Judging 24/100\n",
            "Judging 25/100\n",
            "Judging 26/100\n",
            "Judging 27/100\n",
            "Judging 28/100\n",
            "Judging 29/100\n",
            "Judging 30/100\n",
            "Judging 31/100\n",
            "Judging 32/100\n",
            "Judging 33/100\n",
            "Judging 34/100\n",
            "Judging 35/100\n",
            "Judging 36/100\n",
            "Judging 37/100\n",
            "Judging 38/100\n",
            "Judging 39/100\n",
            "Judging 40/100\n",
            "Judging 41/100\n",
            "Judging 42/100\n",
            "Judging 43/100\n",
            "Judging 44/100\n",
            "Judging 45/100\n",
            "Judging 46/100\n",
            "Judging 47/100\n",
            "Judging 48/100\n",
            "Judging 49/100\n",
            "Judging 50/100\n",
            "Judging 51/100\n",
            "Judging 52/100\n",
            "Judging 53/100\n",
            "Judging 54/100\n",
            "Judging 55/100\n",
            "Judging 56/100\n",
            "Judging 57/100\n",
            "Judging 58/100\n",
            "Judging 59/100\n",
            "Judging 60/100\n",
            "Judging 61/100\n",
            "Judging 62/100\n",
            "Judging 63/100\n",
            "Judging 64/100\n",
            "Judging 65/100\n",
            "Judging 66/100\n",
            "Judging 67/100\n",
            "Judging 68/100\n",
            "Judging 69/100\n",
            "Judging 70/100\n",
            "Judging 71/100\n",
            "Judging 72/100\n",
            "Judging 73/100\n",
            "Judging 74/100\n",
            "Judging 75/100\n",
            "Judging 76/100\n",
            "Judging 77/100\n",
            "Judging 78/100\n",
            "Judging 79/100\n",
            "Judging 80/100\n",
            "Judging 81/100\n",
            "Judging 82/100\n",
            "Judging 83/100\n",
            "Judging 84/100\n",
            "Judging 85/100\n",
            "Judging 86/100\n",
            "Judging 87/100\n",
            "Judging 88/100\n",
            "Judging 89/100\n",
            "Judging 90/100\n",
            "Judging 91/100\n",
            "Judging 92/100\n",
            "Judging 93/100\n",
            "Judging 94/100\n",
            "Judging 95/100\n",
            "Judging 96/100\n",
            "Judging 97/100\n",
            "Judging 98/100\n",
            "Judging 99/100\n",
            "Judging 100/100\n",
            "judged dataset: dataset_with_cot_judged.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualss"
      ],
      "metadata": {
        "id": "3Z2-GLFrxipQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "dataset_path = \"dataset_with_cot_judged.jsonl\"\n",
        "\n",
        "labels = []\n",
        "\n",
        "with open(dataset_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        row = json.loads(line)\n",
        "        labels.append(row.get(\"hallucination_label\"))\n",
        "\n",
        "#count\n",
        "counts = Counter(labels)\n",
        "ordered_labels = [\"0\", \"0.5\", \"1\"]\n",
        "values = [counts.get(lbl, 0) for lbl in ordered_labels]\n",
        "\n",
        "#graph\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(ordered_labels, values)\n",
        "plt.xlabel(\"Hallucination label\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.title(\"Distribution of hallucination(0, 0.5, 1)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "mE9HGvITxl9V",
        "outputId": "c8ba76de-21d4-48b2-b311-d55bfb85ae12"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}